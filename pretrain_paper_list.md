##


### Evaluation Benchmark 


* [arXiv:2406.07450] **Benchmarking Vision-Language Contrastive Methods for Medical Representation Learning**, 
  Shuvendu Roy, Yasaman Parhizkar, Franklin Ogidi, Vahid Reza Khazaie, Michael Colacci, Ali Etemad, Elham Dolatabadi, Arash Afkanpour
  [[Paper](https://arxiv.org/abs/2406.07450)] 

* **OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM**, Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, Ping Luo
  [[Paper](https://arxiv.org/abs/2402.09181)]
  



### Survey and Review 

* [arXiv:2405.12833] **A Survey of Deep Learning-based Radiology Report Generation Using Multimodal Data**,
  Xinyi Wang, Grazziela Figueredo, Ruizhe Li, Wei Emma Zhang, Weitong Chen, Xin Chen
  [[Paper](https://arxiv.org/abs/2405.12833)] 

* [arXiv-2024] **Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review**,
  [[Paper](https://arxiv.org/abs/2403.02469)] 

* **RadLLM: A Comprehensive Healthcare Benchmark of Large Language Models for Radiology**
  [[Paper](https://arxiv.org/pdf/2307.13693.pdf)] 

* **Medical Vision Language Pretraining: A survey**, Prashant Shrestha, Sanskar Amgain, Bidur Khanal, Cristian A. Linte, Binod Bhattarai
  [[Paper](https://arxiv.org/abs/2312.06224)]

* **Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision**, Bobby Azad, Reza Azad, Sania Eskandari, Afshin Bozorgpour, Amirhossein Kazerouni, Islem Rekik, Dorit Merhof
  [[Paper](https://arxiv.org/abs/2310.18689)]

* "**CLIP in Medical Imaging: A Comprehensive Survey.**" Zhao, Zihao, et al.  arXiv preprint arXiv:2312.07353 (2023).
  [[Paper](https://arxiv.org/abs/2312.07353)]
  [[Code](https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging)]

* **Exploring the Boundaries of GPT-4 in Radiology**. Liu, Q., Hyland, S., Bannur, S., Bouzid, K., Castro, D. C., Wetscherek, M. T., ... & Alvarez-Valle, J. (2023).  arXiv preprint arXiv:2310.14573.
  [[Paper](https://arxiv.org/pdf/2310.14573.pdf)]

* "**Deep learning for chest X-ray analysis: A survey.**" Çallı, Erdi, et al.  Medical Image Analysis 72 (2021): 102125.
  [[Paper](https://www.sciencedirect.com/science/article/pii/S1361841521001717)]

* "**A Survey of Large Language Models in Medicine: Progress, Application, and Challenge.**" Zhou, Hongjian, et al.  arXiv preprint arXiv:2311.05112 (2023).
  [[Paper](https://arxiv.org/abs/2311.05112)]
  [[Code](https://github.com/AI-in-Health/MedLLMsPracticalGuide)] 


  

### Year 2024 

* [arXiv:2405.12971] **BiomedParse: a biomedical foundation model for image parsing of everything everywhere all at once**, Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Tristan Naumann, Jianfeng Gao, Angela Crabtree, Jacob Abel, Christine Moung-Wen, Brian Piening, Carlo Bifulco, Mu Wei, Hoifung Poon, Sheng Wang 
  [[Paper](https://arxiv.org/abs/2405.12971)]
  [[Code](https://aka.ms/biomedparse-project)] 

* **Merlin: A Vision Language Foundation Model for 3D Computed Tomography**, Louis Blankemeier, Joseph Paul Cohen, Ashwin Kumar, Dave Van Veen, Syed Jamal Safdar Gardezi, Magdalini Paschali, Zhihong Chen, Jean-Benoit Delbrouck, Eduardo Reis, Cesar Truyts, Christian Bluethgen, Malte Engmann Kjeldskov Jensen, Sophie Ostmeier, Maya Varma, Jeya Maria Jose Valanarasu, Zhongnan Fang, Zepeng Huo, Zaid Nabulsi, Diego Ardila, Wei-Hung Weng, Edson Amaro Junior, Neera Ahuja, Jason Fries, Nigam H. Shah, Andrew Johnston, Robert D. Boutin, Andrew Wentland, Curtis P. Langlotz, Jason Hom, Sergios Gatidis, Akshay S. Chaudhari
  [[Paper](https://arxiv.org/abs/2406.06512)] 

* [arXiv:2405.11793, MICCAI 2024] **MM-Retinal: Knowledge-Enhanced Foundational Pretraining with Fundus Image-Text Expertise**,
  Ruiqi Wu, Chenran Zhang, Jianle Zhang, Yi Zhou, Tao Zhou, Huazhu Fu
  [[Paper](https://arxiv.org/abs/2405.11793)]
  [[Code](https://github.com/lxirich/MM-Retinal)] 

* [arXiv:2405.09586] **Factual Serialization Enhancement: A Key Innovation for Chest X-ray Report Generation**,
  Kang Liu, Zhuoqi Ma, Mengmeng Liu, Zhicheng Jiao, Xiaolu Kang, Qiguang Miao, Kun Xie
  [[Paper](https://arxiv.org/abs/2405.09586)]
  [[Code](https://github.com/mk-runner/FSE)] 

* [arXiv:2405.05237] **EVA-X: A Foundation Model for General Chest X-ray Analysis with Self-supervised Learning**,
  Jingfeng Yao, Xinggang Wang, Yuehao Song, Huangxuan Zhao, Jun Ma, Yajie Chen, Wenyu Liu, Bo Wang
  [[Paper](https://arxiv.org/abs/2405.05237)]
  [[Code](https://github.com/hustvl/EVA-X)] 

* [CVPR-2024] **Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Pre-training Framework**, arXiv:2403.07636, 
  Vu Minh Hieu Phan, Yutong Xie, Yuankai Qi, Lingqiao Liu, Liyang Liu, Bowen Zhang, Zhibin Liao, Qi Wu, Minh-Son To, Johan W. Verjans
  [[Paper](https://arxiv.org/abs/2403.07636)]
  [[Code](https://github.com/HieuPhan33/MAVL)] 

* **Advancing human-centric AI for robust X-ray analysis through holistic self-supervised learning**,
  Théo Moutakanni, Piotr Bojanowski, Guillaume Chassagnon, Céline Hudelot, Armand Joulin, Yann LeCun, Matthew Muckley, Maxime Oquab, Marie-Pierre Revel, Maria Vakalopoulou
  [[Paper](https://arxiv.org/abs/2405.01469)] 

* **MEDITRON-70B: Scaling Medical Pretraining for Large Language Models**,
  Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, Antoine Bosselut
  [[Paper](https://arxiv.org/abs/2311.16079)] 
  [[Code](https://github.com/epfLLM/meditron)]
  [[meditron-7b](https://huggingface.co/epfl-llm/meditron-7b)]
  [[meditron-70b](https://huggingface.co/epfl-llm/meditron-70b)]

* **Capabilities of Gemini Models in Medicine**, Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, Juanma Zambrano Chaves, Szu-Yeu Hu, et al.
  [[Paper](https://arxiv.org/abs/2404.18416)] 

* [CVPR 2024] **Low-Rank Knowledge Decomposition for Medical Foundation Models**, arXiv:2404.17184, 
  Yuhang Zhou, Haolin Li, Siyuan Du, Jiangchao Yao, Ya Zhang, Yanfeng Wang
  [[Paper](https://arxiv.org/abs/2404.17184)] 

* **MeDSLIP: Medical Dual-Stream Language-Image Pre-training for Fine-grained Alignment**, Wenrui Fan, Mohammod Naimul Islam Suvon, Shuo Zhou, Xianyuan Liu, Samer Alabed, Venet Osmani, Andrew Swift, Chen Chen, Haiping Lu
  [[Paper](https://arxiv.org/abs/2403.10635)]
  [[]()]

* **Anatomical Structure-Guided Medical Vision-Language Pre-training**, 
  Qingqiu Li, Xiaohan Yan, Jilan Xu, Runtian Yuan, Yuejie Zhang, Rui Feng, Quanli Shen, Xiaobo Zhang, Shujun Wang
[[Paper](https://arxiv.org/abs/2403.09294)]

* **MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder**, Lei Li, Tianfang Zhang, Xinglin Zhang, Jiaqi Liu, Bingqi Ma, Yan Luo, Tao Chen
  [[Paper](https://arxiv.org/abs/2403.04626)] 

* **OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine**, Xiaosong Wang, Xiaofan Zhang, Guotai Wang, Junjun He, Zhongyu Li, Wentao Zhu, Yi Guo, Qi Dou, Xiaoxiao Li, Dequan Wang, Liang Hong, Qicheng Lao, Tong Ruan, Yukun Zhou, Yixue Li, Jie Zhao, Kang Li, Xin Sun, Lifeng Zhu, Shaoting Zhang
  [[Paper](https://arxiv.org/abs/2402.18028)]
  [[Code](https://github.com/openmedlab)] 

* **"Towards Building Multilingual Language Model for Medicine."** Qiu, Pengcheng, et al. arXiv preprint arXiv:2402.13963 (2024).
  [[Paper](https://arxiv.org/abs/2402.13963)]
  [[Code](https://github.com/MAGIC-AI4Med/MMedLM)] 

* **MLIP: Enhancing Medical Visual Representation with Divergence Encoder and Knowledge-guided Contrastive Learning**, Zhe Li, Laurence T. Yang, Bocheng Ren, Xin Nie, Zhangyang Gao, Cheng Tan, Stan Z. Li
  [[Paper](https://arxiv.org/abs/2402.02045)]

* "**MLIP: Medical Language-Image Pre-training with Masked Local Representation Learning.**" Liu, Jiarun, et al.  arXiv preprint arXiv:2401.01591 (2024). [[Paper](https://arxiv.org/pdf/2401.01591.pdf)] 

* **CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation**, Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, Emily B. Tsai, Andrew Johnston, Cameron Olsen, Tanishq Mathew Abraham, Sergios Gatidis, Akshay S. Chaudhari, Curtis Langlotz 
  [[Paper](https://arxiv.org/abs/2401.12208)]
  [[Project](https://stanford-aimi.github.io/chexagent.html)]
  [[Demo](http://34.31.232.110:8888/)]
  [[HuggingFace](https://huggingface.co/StanfordAIMI/CheXagent-8b/tree/main)]
  [[Github](https://github.com/Stanford-AIMI/CheXagent)]

* **Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models**,
  [[Paper](https://arxiv.org/pdf/2401.12215.pdf)]
  [[Code](https://github.com/RL4M/MED-PEFT)] 



### Year 2023 


* [IEEE TMI 2023] **Improving Medical Vision-Language Contrastive Pretraining With Semantics-Aware Triage**,
  [[Paper](https://ieeexplore.ieee.org/abstract/document/10182304)] 

* [ICCV-2023] **Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts**, Zhihong Chen1,2∗ Shizhe Diao3∗ Benyou Wang1,2† Guanbin Li4† Xiang Wan
  [[Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Towards_Unifying_Medical_Vision-and-Language_Pre-Training_via_Soft_Prompts_ICCV_2023_paper.pdf)]
  [[Code](https://github.com/zhjohnchan/ptunifier)]

* [arXiv-2023] **ECAMP: Entity-centered Context-aware Medical Vision Language Pre-training**, Rongsheng Wang, Qingsong Yao, Haoran Lai, Zhiyang He, Xiaodong Tao, Zihang Jiang, S.Kevin Zhou
  [[Paper](https://arxiv.org/abs/2312.13316)]
  [[Code](https://github.com/ToniChopp/ECAMP)]

* [MICCAI-2022] **Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training**, Zhihong Chen, Yuhao Du, Jinpeng Hu, Yang Liu, Guanbin Li, Xiang Wan, and Tsung-Hui Chang
  [[Paper](https://arxiv.org/pdf/2209.07098.pdf)]
  [[Code](https://github.com/zhjohnchan/M3AE)]

* **Advancing Radiograph Representation Learning with Masked Record Modeling**. Zhou, Hong-Yu, et al. The Eleventh International Conference on Learning Representations. 2022.
  [[Paper](https://openreview.net/pdf?id=w-x7U26GM7j)]
  [[Code](https://github.com/RL4M/MRM-pytorch)]

* **GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition**, Shih-Cheng Huang* Liyue Shen* Matthew P. Lungren Serena Yeung
  [[Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_GLoRIA_A_Multimodal_Global-Local_Representation_Learning_Framework_for_Label-Efficient_Medical_ICCV_2021_paper.pdf)]
  [[Code](https://github.com/marshuang80/gloria)]

* **G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training.** Liu, C., Ouyang, C., Cheng, S., Shah, A., Bai, W., & Arcucci, R. (2023).  arXiv preprint arXiv:2312.01522.
  [[Paper](https://arxiv.org/abs/2312.01522)]
  [[Code]()]

* **UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts**. Zhan, C., Zhang, Y., Lin, Y., Wang, G., & Wang, H. (2023).  arXiv preprint arXiv:2312.11171.
  [[Paper](https://arxiv.org/abs/2312.11171)]

* **T3D: Towards 3D Medical Image Understanding through Vision-Language Pre-training**. Liu, C., Ouyang, C., Chen, Y., Quilodrán-Casas, C. C., Ma, L., Fu, J., ... & Arcucci, R. (2023).  arXiv preprint arXiv:2312.01529.
  [[Paper](https://arxiv.org/abs/2312.01529)]

* "**Fine-grained Medical Vision-Language Representation Learning for Radiology Report Generation**." Wang, Siyuan, et al.  Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.
  [[Paper](https://aclanthology.org/2023.emnlp-main.989.pdf)]

* **"Enhancing representation in radiography-reports foundation model: A granular alignment algorithm using masked contrastive learning."** Huang, Weijian, et al.  arXiv preprint arXiv:2309.05904 (2023).
  [[Paper](https://arxiv.org/abs/2309.05904)]

* "**Multi-task paired masking with alignment modeling for medical vision-language pre-training**." Zhang, Ke, et al.  IEEE Transactions on Multimedia (2023).
  [[Paper](https://ieeexplore.ieee.org/abstract/document/10288259/)]

* "**Cxr-clip: Toward large scale chest x-ray language-image pre-training.**" You, Kihyun, et al.  International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023.
  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-43895-0_10)]
  [[Code](https://github.com/kakaobrain/cxr-clip)]

* "**A medical multimodal large language model for future pandemics.**" Liu, Fenglin, et al.  NPJ Digital Medicine 6.1 (2023): 226.
  [[Paper](https://www.nature.com/articles/s41746-023-00952-2.pdf)]

* **Imitate: Clinical prior guided hierarchical vision-language pre-training**. Liu, C., Cheng, S., Shi, M., Shah, A., Bai, W., & Arcucci, R. (2023).  arXiv preprint arXiv:2310.07355.
  [[Paper](https://arxiv.org/abs/2310.07355)]

* **Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation**, Wenting Chen, Xiang Li, Linlin Shen, Yixuan Yuan
  [[Paper](https://arxiv.org/abs/2312.08078)]

* "**Pre-trained Universal Medical Image Transformer.**" Luo, Lingxiao, et al.  arXiv preprint arXiv:2312.07630 (2023).
  [[Paper](https://arxiv.org/abs/2312.07630)]
  [[Code](https://github.com/function2-llx/PUMIT)]

* **MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training for X-ray Diagnosis**, ICCV-2023, 
  [[Project](https://chaoyi-wu.github.io/MedKLIP/)]
  [[Github](https://github.com/MediaBrain-SJTU/MedKLIP)]

* **"Delving into masked autoencoders for multi-label thorax disease classification."** Xiao, Junfei, et al.  Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023.
  [[Paper](https://arxiv.org/abs/2210.12843)]
  [[Code](https://github.com/lambert-x/Medical_MAE)]

* **MAIRA-1: A specialised large multimodal model for radiology report generation**, Stephanie Hyland, Shruthi Bannur, Kenza Bouzid, Daniel Coelho de Castro, Mercy Ranjit, Anton Schwaighofer, Fernando Pérez-García, Valentina Salvatelli, Shaury Srivastav, Anja Thieme, Noel Codella, Matthew P Lungren, Maria Teodora Wetscherek, Ozan Oktay, Javier Alvarez-Valle 
  [[Paper](https://arxiv.org/abs/2311.13668)]

* 
 




### Year 2022 
* [ACMMM-2022] ARL: **Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge**, Zhihong Chen, Guanbin Li, Xiang Wan
  [[Paper](https://arxiv.org/abs/2209.07118)]
  [[Code](https://github.com/zhjohnchan/ARL)]

* Zhang, Yuhao, et al. "**Contrastive learning of medical visual representations from paired images and text.**" Machine Learning for Healthcare Conference. PMLR, 2022.
  [[Paper](https://proceedings.mlr.press/v182/zhang22a/zhang22a.pdf)]
  [[Code](https://github.com/yuhaozhang/convirt)]


### Year 2021 
* 
































































































