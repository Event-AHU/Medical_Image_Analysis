##


### Evaluation Benchmark 

* **OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM**, Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, Ping Luo
  [[Paper](https://arxiv.org/abs/2402.09181)]
  

### Survey and Review 

* [arXiv-2024] **Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review**,
  [[Paper](https://arxiv.org/abs/2403.02469)] 

* **RadLLM: A Comprehensive Healthcare Benchmark of Large Language Models for Radiology**
  [[Paper](https://arxiv.org/pdf/2307.13693.pdf)] 

* **Medical Vision Language Pretraining: A survey**, Prashant Shrestha, Sanskar Amgain, Bidur Khanal, Cristian A. Linte, Binod Bhattarai
  [[Paper](https://arxiv.org/abs/2312.06224)]

* **Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision**, Bobby Azad, Reza Azad, Sania Eskandari, Afshin Bozorgpour, Amirhossein Kazerouni, Islem Rekik, Dorit Merhof
  [[Paper](https://arxiv.org/abs/2310.18689)]

* "**CLIP in Medical Imaging: A Comprehensive Survey.**" Zhao, Zihao, et al.  arXiv preprint arXiv:2312.07353 (2023).
  [[Paper](https://arxiv.org/abs/2312.07353)]
  [[Code](https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging)]

* **Exploring the Boundaries of GPT-4 in Radiology**. Liu, Q., Hyland, S., Bannur, S., Bouzid, K., Castro, D. C., Wetscherek, M. T., ... & Alvarez-Valle, J. (2023).  arXiv preprint arXiv:2310.14573.
  [[Paper](https://arxiv.org/pdf/2310.14573.pdf)]

* "**Deep learning for chest X-ray analysis: A survey.**" Çallı, Erdi, et al.  Medical Image Analysis 72 (2021): 102125.
  [[Paper](https://www.sciencedirect.com/science/article/pii/S1361841521001717)]

* "**A Survey of Large Language Models in Medicine: Progress, Application, and Challenge.**" Zhou, Hongjian, et al.  arXiv preprint arXiv:2311.05112 (2023).
  [[Paper](https://arxiv.org/abs/2311.05112)]
  [[Code](https://github.com/AI-in-Health/MedLLMsPracticalGuide)] 


  

### Year 2024 

* **Capabilities of Gemini Models in Medicine**, Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, Juanma Zambrano Chaves, Szu-Yeu Hu, et al.
  [[Paper](https://arxiv.org/abs/2404.18416)] 

* [CVPR 2024] **Low-Rank Knowledge Decomposition for Medical Foundation Models**, arXiv:2404.17184, 
  Yuhang Zhou, Haolin Li, Siyuan Du, Jiangchao Yao, Ya Zhang, Yanfeng Wang
  [[Paper](https://arxiv.org/abs/2404.17184)] 

* **MeDSLIP: Medical Dual-Stream Language-Image Pre-training for Fine-grained Alignment**, Wenrui Fan, Mohammod Naimul Islam Suvon, Shuo Zhou, Xianyuan Liu, Samer Alabed, Venet Osmani, Andrew Swift, Chen Chen, Haiping Lu
  [[Paper](https://arxiv.org/abs/2403.10635)]
  [[]()]

* **Anatomical Structure-Guided Medical Vision-Language Pre-training**, 
  Qingqiu Li, Xiaohan Yan, Jilan Xu, Runtian Yuan, Yuejie Zhang, Rui Feng, Quanli Shen, Xiaobo Zhang, Shujun Wang
[[Paper](https://arxiv.org/abs/2403.09294)]

* **MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder**, Lei Li, Tianfang Zhang, Xinglin Zhang, Jiaqi Liu, Bingqi Ma, Yan Luo, Tao Chen
  [[Paper](https://arxiv.org/abs/2403.04626)] 

* **OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine**, Xiaosong Wang, Xiaofan Zhang, Guotai Wang, Junjun He, Zhongyu Li, Wentao Zhu, Yi Guo, Qi Dou, Xiaoxiao Li, Dequan Wang, Liang Hong, Qicheng Lao, Tong Ruan, Yukun Zhou, Yixue Li, Jie Zhao, Kang Li, Xin Sun, Lifeng Zhu, Shaoting Zhang
  [[Paper](https://arxiv.org/abs/2402.18028)]
  [[Code](https://github.com/openmedlab)] 

* **"Towards Building Multilingual Language Model for Medicine."** Qiu, Pengcheng, et al. arXiv preprint arXiv:2402.13963 (2024).
  [[Paper](https://arxiv.org/abs/2402.13963)]
  [[Code](https://github.com/MAGIC-AI4Med/MMedLM)] 

* **MLIP: Enhancing Medical Visual Representation with Divergence Encoder and Knowledge-guided Contrastive Learning**, Zhe Li, Laurence T. Yang, Bocheng Ren, Xin Nie, Zhangyang Gao, Cheng Tan, Stan Z. Li
  [[Paper](https://arxiv.org/abs/2402.02045)]

* "**MLIP: Medical Language-Image Pre-training with Masked Local Representation Learning.**" Liu, Jiarun, et al.  arXiv preprint arXiv:2401.01591 (2024). [[Paper](https://arxiv.org/pdf/2401.01591.pdf)] 

* **CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation**, Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, Emily B. Tsai, Andrew Johnston, Cameron Olsen, Tanishq Mathew Abraham, Sergios Gatidis, Akshay S. Chaudhari, Curtis Langlotz 
  [[Paper](https://arxiv.org/abs/2401.12208)]
  [[Project](https://stanford-aimi.github.io/chexagent.html)]
  [[Demo](http://34.31.232.110:8888/)]
  [[HuggingFace](https://huggingface.co/StanfordAIMI/CheXagent-8b/tree/main)]
  [[Github](https://github.com/Stanford-AIMI/CheXagent)]

* **Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models**,
  [[Paper](https://arxiv.org/pdf/2401.12215.pdf)]
  [[Code](https://github.com/RL4M/MED-PEFT)] 



### Year 2023 

* [ICCV-2023] **Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts**, Zhihong Chen1,2∗ Shizhe Diao3∗ Benyou Wang1,2† Guanbin Li4† Xiang Wan
  [[Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Towards_Unifying_Medical_Vision-and-Language_Pre-Training_via_Soft_Prompts_ICCV_2023_paper.pdf)]
  [[Code](https://github.com/zhjohnchan/ptunifier)]

* [arXiv-2023] **ECAMP: Entity-centered Context-aware Medical Vision Language Pre-training**, Rongsheng Wang, Qingsong Yao, Haoran Lai, Zhiyang He, Xiaodong Tao, Zihang Jiang, S.Kevin Zhou
  [[Paper](https://arxiv.org/abs/2312.13316)]
  [[Code](https://github.com/ToniChopp/ECAMP)]

* [MICCAI-2022] **Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training**, Zhihong Chen, Yuhao Du, Jinpeng Hu, Yang Liu, Guanbin Li, Xiang Wan, and Tsung-Hui Chang
  [[Paper](https://arxiv.org/pdf/2209.07098.pdf)]
  [[Code](https://github.com/zhjohnchan/M3AE)]

* **Advancing Radiograph Representation Learning with Masked Record Modeling**. Zhou, Hong-Yu, et al. The Eleventh International Conference on Learning Representations. 2022.
  [[Paper](https://openreview.net/pdf?id=w-x7U26GM7j)]
  [[Code](https://github.com/RL4M/MRM-pytorch)]

* **GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition**, Shih-Cheng Huang* Liyue Shen* Matthew P. Lungren Serena Yeung
  [[Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_GLoRIA_A_Multimodal_Global-Local_Representation_Learning_Framework_for_Label-Efficient_Medical_ICCV_2021_paper.pdf)]
  [[Code](https://github.com/marshuang80/gloria)]

* **G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training.** Liu, C., Ouyang, C., Cheng, S., Shah, A., Bai, W., & Arcucci, R. (2023).  arXiv preprint arXiv:2312.01522.
  [[Paper](https://arxiv.org/abs/2312.01522)]
  [[Code]()]

* **UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts**. Zhan, C., Zhang, Y., Lin, Y., Wang, G., & Wang, H. (2023).  arXiv preprint arXiv:2312.11171.
  [[Paper](https://arxiv.org/abs/2312.11171)]

* **T3D: Towards 3D Medical Image Understanding through Vision-Language Pre-training**. Liu, C., Ouyang, C., Chen, Y., Quilodrán-Casas, C. C., Ma, L., Fu, J., ... & Arcucci, R. (2023).  arXiv preprint arXiv:2312.01529.
  [[Paper](https://arxiv.org/abs/2312.01529)]

* "**Fine-grained Medical Vision-Language Representation Learning for Radiology Report Generation**." Wang, Siyuan, et al.  Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.
  [[Paper](https://aclanthology.org/2023.emnlp-main.989.pdf)]

* **"Enhancing representation in radiography-reports foundation model: A granular alignment algorithm using masked contrastive learning."** Huang, Weijian, et al.  arXiv preprint arXiv:2309.05904 (2023).
  [[Paper](https://arxiv.org/abs/2309.05904)]

* "**Multi-task paired masking with alignment modeling for medical vision-language pre-training**." Zhang, Ke, et al.  IEEE Transactions on Multimedia (2023).
  [[Paper](https://ieeexplore.ieee.org/abstract/document/10288259/)]

* "**Cxr-clip: Toward large scale chest x-ray language-image pre-training.**" You, Kihyun, et al.  International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2023.
  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-43895-0_10)]
  [[Code](https://github.com/kakaobrain/cxr-clip)]

* "**A medical multimodal large language model for future pandemics.**" Liu, Fenglin, et al.  NPJ Digital Medicine 6.1 (2023): 226.
  [[Paper](https://www.nature.com/articles/s41746-023-00952-2.pdf)]

* **Imitate: Clinical prior guided hierarchical vision-language pre-training**. Liu, C., Cheng, S., Shi, M., Shah, A., Bai, W., & Arcucci, R. (2023).  arXiv preprint arXiv:2310.07355.
  [[Paper](https://arxiv.org/abs/2310.07355)]

* **Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation**, Wenting Chen, Xiang Li, Linlin Shen, Yixuan Yuan
  [[Paper](https://arxiv.org/abs/2312.08078)]

* "**Pre-trained Universal Medical Image Transformer.**" Luo, Lingxiao, et al.  arXiv preprint arXiv:2312.07630 (2023).
  [[Paper](https://arxiv.org/abs/2312.07630)]
  [[Code](https://github.com/function2-llx/PUMIT)]

* **MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training for X-ray Diagnosis**, ICCV-2023, 
  [[Project](https://chaoyi-wu.github.io/MedKLIP/)]
  [[Github](https://github.com/MediaBrain-SJTU/MedKLIP)]

* **"Delving into masked autoencoders for multi-label thorax disease classification."** Xiao, Junfei, et al.  Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023.
  [[Paper](https://arxiv.org/abs/2210.12843)]
  [[Code](https://github.com/lambert-x/Medical_MAE)]

* **MAIRA-1: A specialised large multimodal model for radiology report generation**, Stephanie Hyland, Shruthi Bannur, Kenza Bouzid, Daniel Coelho de Castro, Mercy Ranjit, Anton Schwaighofer, Fernando Pérez-García, Valentina Salvatelli, Shaury Srivastav, Anja Thieme, Noel Codella, Matthew P Lungren, Maria Teodora Wetscherek, Ozan Oktay, Javier Alvarez-Valle 
  [[Paper](https://arxiv.org/abs/2311.13668)]

* 
 




### Year 2022 
* [ACMMM-2022] ARL: **Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge**, Zhihong Chen, Guanbin Li, Xiang Wan
  [[Paper](https://arxiv.org/abs/2209.07118)]
  [[Code](https://github.com/zhjohnchan/ARL)]

* Zhang, Yuhao, et al. "**Contrastive learning of medical visual representations from paired images and text.**" Machine Learning for Healthcare Conference. PMLR, 2022.
  [[Paper](https://proceedings.mlr.press/v182/zhang22a/zhang22a.pdf)]
  [[Code](https://github.com/yuhaozhang/convirt)]


### Year 2021 
* 
































































































